{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7398091e",
   "metadata": {},
   "source": [
    "### Code to be followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20401332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Random seed \"\"\"\n",
    "from transformers import set_seed\n",
    "set_seed(11207330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfef332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load dataset\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def load_data(dataset_name_or_path: str, \n",
    "              prompt_template_path: str = None):\n",
    "    \"\"\" Init: multiple choices answer \"\"\"\n",
    "    all_choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    \n",
    "    \"\"\" Init: prompt template\"\"\"\n",
    "    if prompt_template_path is not None:\n",
    "        try:\n",
    "            with Path(prompt_template_path).open(\"r\", encoding=\"utf-8\") as file:\n",
    "                prompt_template = file.read()\n",
    "        except (FileNotFoundError, IOError) as e:\n",
    "            raise RuntimeError(f\"Failed to load the prompt template: {e}\") from e\n",
    "    else:\n",
    "        prompt_template = \"{question}\"\n",
    "\n",
    "    \n",
    "    \"\"\" preprocess function: use the prompt template to format the question \"\"\"\n",
    "    def _preprocess(example):\n",
    "        example[\"question\"] = (\n",
    "            f\"{example['instruction']}\\n\"\n",
    "            + f\"{example['question']}\\n\"\n",
    "            + \"\".join(example[f\"option{i + 1}\"] for i in range(len(all_choices)))\n",
    "        )\n",
    "        example[\"answer\"] = example[\"answer\"].replace(\"A\", \"0\").replace(\"B\", \"1\").replace(\"C\", \"2\").replace(\"D\", \"3\") # anwer to index: 這邊是用 0,1,2,3來表示答案\n",
    "        example[\"question\"] = prompt_template.format(question=example[\"question\"])\n",
    "        return example\n",
    "    \n",
    "    dataset = load_dataset(\n",
    "                    \"json\",\n",
    "                    data_files=dataset_name_or_path,\n",
    "                    split=\"train\",\n",
    "                )\n",
    "    \n",
    "    return_dataset = Dataset.from_list([\n",
    "                _preprocess(example)\n",
    "                for example in tqdm(dataset, desc=\"Processing dataset\", unit=\"example\")\n",
    "            ])\n",
    "    return return_dataset\n",
    "\n",
    "load_data(dataset_name_or_path=\"TOCFL-MultiBench/TOCFL-MultiBench.json\", prompt_template_path=\"prompt/base.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c11633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" metrics\"\"\"\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def calculate_metrics(\n",
    "    all_choices: list,\n",
    "    all_answers: list,\n",
    "    all_response: list,\n",
    "    all_index2ans: list = None,\n",
    "    allow_random: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"calculate_metrics\"\"\"\n",
    "    if all_index2ans is None:\n",
    "        all_index2ans = [None] * len(all_response)\n",
    "\n",
    "    predictions = [\n",
    "        parse_multi_choice_response(response, all_choices, index2ans, allow_random)\n",
    "        for response, index2ans in zip(all_response, all_index2ans)\n",
    "    ]\n",
    "\n",
    "    accuracy = accuracy_score(all_answers, predictions)\n",
    "    f1 = f1_score(all_answers, predictions, average=\"weighted\", zero_division=1)\n",
    "    precision = precision_score(all_answers, predictions, average=\"weighted\", zero_division=1)\n",
    "    recall = recall_score(all_answers, predictions, average=\"weighted\", zero_division=1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "def parse_multi_choice_response(\n",
    "    response: str,\n",
    "    all_choices: list = [\"A\", \"B\", \"C\", \"D\"],\n",
    "    index2ans: dict = None,\n",
    "    allow_random: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"parse_multi_choice_response\"\"\"\n",
    "    for char in [',', '.', '!', '?', ';', ':', \"'\"]:\n",
    "        response = response.strip(char)\n",
    "    response = \" \" + response + \" \"\n",
    "\n",
    "    index_ans = True\n",
    "    ans_with_brack = False\n",
    "    candidates = []\n",
    "    for choice in all_choices:\n",
    "        if f'({choice})' in response:\n",
    "            candidates.append(choice)\n",
    "            ans_with_brack = True\n",
    "\n",
    "    if len(candidates) == 0:\n",
    "        for choice in all_choices:\n",
    "            if f' {choice} ' in response:\n",
    "                candidates.append(choice)\n",
    "\n",
    "    if index2ans is not None and len(candidates) == 0 and len(response.split()) > 5:\n",
    "        for index, ans in index2ans.items():\n",
    "            if ans and ans.lower() in response.lower():\n",
    "                candidates.append(index)\n",
    "                index_ans = False\n",
    "\n",
    "    if len(candidates) == 0:\n",
    "        if allow_random:\n",
    "            pred_index = random.choice(all_choices)\n",
    "        else:\n",
    "            pred_index = \"\"\n",
    "\n",
    "    elif len(candidates) > 1:\n",
    "        start_indexes = []\n",
    "        if index_ans:\n",
    "            if ans_with_brack:\n",
    "                for can in candidates:\n",
    "                    index = response.rfind(f'({can})')\n",
    "                    start_indexes.append(index)\n",
    "            else:\n",
    "                for can in candidates:\n",
    "                    index = response.rfind(f\" {can} \")\n",
    "                    start_indexes.append(index)\n",
    "        else:\n",
    "            for can in candidates:\n",
    "                index = response.lower().rfind(index2ans[can].lower())\n",
    "                start_indexes.append(index)\n",
    "\n",
    "        pred_index = candidates[np.argmax(start_indexes)]\n",
    "    else:\n",
    "        pred_index = candidates[0]\n",
    "\n",
    "    return pred_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479196d7",
   "metadata": {},
   "source": [
    "### Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa80797",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(dataset_name_or_path=\"TOCFL-MultiBench/TOCFL-MultiBench.json\", prompt_template_path=\"prompt/base.txt\")\n",
    "\n",
    "\"\"\" Usage: 問題底家 \"\"\"\n",
    "print(data)\n",
    "print(data['question'])\n",
    "print(data['answer']) # answer 被弄成數值，以搭配calculate_metrics。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003485af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 定義可選項目（模型可能輸出的 label）\n",
    "all_choices   = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "# 2. 定義「正確答案」列表（ground truth）\n",
    "#    比方說我們有四題，正確答案分別是 A, B, C, A\n",
    "all_answers   = [\"A\", \"B\", \"C\", \"A\"]\n",
    "\n",
    "# 3. 定義模型回傳的「原始字串」列表\n",
    "#    這裡假設模型回了跟正確一樣的 four responses\n",
    "all_response  = [\"A\", \"B\", \"C\", \"A\"]\n",
    "\n",
    "# 4. （選擇性）定義 index2ans 映射\n",
    "#    當模型回的是文字（例如 \"dog\"、\"cat\"）而非 A/B/C/D 時，用這個 dict 幫它對回標籤。\n",
    "#    key 是選項字母，value 是對應的文字答案。\n",
    "#    如果你的模型只回 A/B/C/D，就可以不傳這個參數（預設會是全 None）。\n",
    "all_index2ans = None\n",
    "\n",
    "# 呼叫 calculate_metrics\n",
    "metrics = calculate_metrics(\n",
    "    all_choices=all_choices,\n",
    "    all_answers=all_answers,\n",
    "    all_response=all_response,\n",
    "    all_index2ans=all_index2ans,\n",
    "    allow_random=True,         # 若 parse 不出答案，是否隨機選一個\n",
    ")\n",
    "\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025-03-16-decoding-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
